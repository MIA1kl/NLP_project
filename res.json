{
  "1": {
    "headline": "AI algorithm unblurs the cosmos",
    "subtitle": "Tool produces faster, more realistic images than current methods",
    "date_posted": "March 31, 2023",
    "source": "Northwestern University",
    "summary": "Researchers adapted a well-known computer-vision algorithm used for sharpening photos and, for the first time, applied it to astronomical images from ground-based telescopes. While astrophysicists already use technologies to remove blur, the adapted AI-driven algorithm works faster and produces more realistic images than current technologies. The resulting images are blur-free and truer to life.",
    "text": "Even images obtained by the world's best ground-based telescopes are blurry due to the atmosphere's shifting pockets of air. While seemingly harmless, this blur obscures the shapes of objects in astronomical images, sometimes leading to error-filled physical measurements that are essential for understanding the nature of our universe.\nNow researchers at Northwestern University and Tsinghua University in Beijing have unveiled a new strategy to fix this issue. The team adapted a well-known computer-vision algorithm used for sharpening photos and, for the first time, applied it to astronomical images from ground-based telescopes. The researchers also trained the artificial intelligence (AI) algorithm on data simulated to match the Vera C. Rubin Observatory's imaging parameters, so, when the observatory opens next year, the tool will be instantly compatible.\nWhile astrophysicists already use technologies to remove blur, the adapted AI-driven algorithm works faster and produces more realistic images than current technologies. The resulting images are blur-free and truer to life. They also are beautiful -- although that's not the technology's purpose.\n\"Photography's goal is often to get a pretty, nice-looking image,\" said Northwestern's Emma Alexander, the study's senior author. \"But astronomical images are used for science. By cleaning up images in the right way, we can get more accurate data. The algorithm removes the atmosphere computationally, enabling physicists to obtain better scientific measurements. At the end of the day, the images do look better as well.\"\nThe research will be published March 30 in the Monthly Notices of the Royal Astronomical Society.\nAlexander is an assistant professor of computer science at Northwestern's McCormick School of Engineering, where she runs the Bio Inspired Vision Lab. She co-led the new study with Tianao Li, an undergraduate in electrical engineering at Tsinghua University and a research intern in Alexander's lab.\nWhen light emanates from distant stars, planets and galaxies, it travels through Earth's atmosphere before it hits our eyes. Not only does our atmosphere block out certain wavelengths of light, it also distorts the light that reaches Earth. Even clear night skies still contain moving air that affects light passing through it. That's why stars twinkle and why the best ground-based telescopes are located at high altitudes where the atmosphere is thinnest.\n\"It's a bit like looking up from the bottom of a swimming pool,\" Alexander said. \"The water pushes light around and distorts it. The atmosphere is, of course, much less dense, but it's a similar concept.\"\nThe blur becomes an issue when astrophysicists analyze images to extract cosmological data. By studying the apparent shapes of galaxies, scientists can detect the gravitational effects of large-scale cosmological structures, which bend light on its way to our planet. This can cause an elliptical galaxy to appear rounder or more stretched than it really is. But atmospheric blur smears the image in a way that warps the galaxy shape. Removing the blur enables scientists to collect accurate shape data.\n\"Slight differences in shape can tell us about gravity in the universe,\" Alexander said. \"These differences are already difficult to detect. If you look at an image from a ground-based telescope, a shape might be warped. It's hard to know if that's because of a gravitational effect or the atmosphere.\"\nTo tackle this challenge, Alexander and Li combined an optimization algorithm with a deep-learning network trained on astronomical images. Among the training images, the team included simulated data that matches the Rubin Observatory's expected imaging parameters. The resulting tool produced images with 38.6% less error compared to classic methods for removing blur and 7.4% less error compared to modern methods.\nWhen the Rubin Observatory officially opens next year, its telescopes will begin a decade-long deep survey across an enormous portion of the night sky. Because the researchers trained the new tool on data specifically designed to simulate Rubin's upcoming images, it will be able to help analyze the survey's highly anticipated data.\nFor astronomers interested in using the tool, the open-source, user-friendly code and accompanying tutorials are available online.\n\"Now we pass off this tool, putting it into the hands of astronomy experts,\" Alexander said. \"We think this could be a valuable resource for sky surveys to obtain the most realistic data possible.\"\nThe study, \"Galaxy image deconvolution for weak gravitational lensing with unrolled plug-and-play ADMM,\" used computational resources from the Computational Photography Lab at Northwestern University."
  },
  "2": {
    "headline": "New details of SARS-COV-2 structure",
    "subtitle": "Computational modeling reveals finer view of particle's outer membrane, which could lead to more effective therapies and vaccines",
    "date_posted": "March 30, 2023",
    "source": "Worcester Polytechnic Institute",
    "summary": "Researchers used computational modeling to reveal finer details surrounding the outer shell of the COVID-19 virus. The work expands the scientific community's understanding of SARS-COV-2, and could lead to more refined antiviral therapies and better vaccines.",
    "text": "\"This is critical knowledge we need to fight future pandemics,\" said Dmitry Korkin, Harold L. Jurist '61 and Heather E. Jurist Dean's Professor of Computer Science and lead researcher on the project. \"Understanding the SARS-COV-2 virus envelope should allow us to model the actual process of the virus attaching to the cell and apply this knowledge to our understanding of the therapies at the molecular level. For instance, how can the viral activity be inhibited by antiviral drugs? How much antiviral blocking is needed to prevent virus-to-host interaction? We don't know. But this is the best thing we can do right now -- to be able to simulate actual processes.\"\nFeeding genetic sequencing information and massive amounts of real-world data about the pandemic virus into a supercomputer in Texas, Korkin and his team, working in partnership with a group led by Siewert-Jan Marrink at the University of Groningen, Netherlands, produced a computational model of the virus's envelope, or outer shell, in \"near atomistic detail\" that had until now been beyond the reach of even the most powerful microscopes and imaging techniques.\nEssentially, the computer used structural bioinformatics and computational biophysics to create its own picture of what the SARS-COV-2 particle looks like. And that picture showed that the virus is more elliptical than spherical and can change its shape. Korkin said the work also led to a better understanding of the M proteins in particular: underappreciated and overlooked components of the virus's envelope.\nThe M proteins form entities called dimers with a copy of each other, and play a role in the particle's shape-shifting by keeping the structure flexible overall while providing a triangular mesh-like structure on the interior that makes it remarkably resilient, Korkin said. In contrast, on the exterior, the proteins assemble into mysterious filament-like structures that have puzzled scientists who have seen Korkin's results, and will require further study.\nKorkin said the structural model developed by the researchers expands what was already known about the envelope architecture of the SARS-COV-2 virus and previous SARS- and MERS-related outbreaks. The computational protocol used to create the model could also be applied to more rapidly model future coronaviruses, he said. A clearer picture of the virus' structure could reveal crucial vulnerabilities.\n\"The envelope properties of SARS-COV-2 are likely to be similar to other coronaviruses,\" he said. \"Eventually, knowledge about the properties of coronavirus membrane proteins could lead to new therapies and vaccines for future viruses.\"\nThe new findings published in Structure were three years in the making and built upon Korkin's work in the early days of the pandemic to provide the first 3D roadmap of the virus, based on genetic sequence information from the first isolated strain in China."
  },
  "3": {
    "headline": "Can AI predict how you'll vote in the next election?",
    "subtitle": "Study proves artificial intelligence can respond to complex survey questions like a real human",
    "date_posted": "March 30, 2023",
    "source": "Brigham Young University",
    "summary": "Artificial intelligence technologies like ChatGPT are seemingly doing everything these days: writing code, composing music, and even creating images so realistic you'll think they were taken by professional photographers. Add thinking and responding like a human to the conga line of capabilities. A recent study proves that artificial intelligence can respond to complex survey questions just like a real human.",
    "text": "To determine the possibility of using artificial intelligence as a substitute for human responders in survey-style research, a team of political science and computer science professors and graduate students at BYU tested the accuracy of programmed algorithms of a GPT-3 language model -- a model that mimics the complicated relationship between human ideas, attitudes, and sociocultural contexts of subpopulations.\nIn one experiment, the researchers created artificial personas by assigning the AI certain characteristics like race, age, ideology, and religiosity; and then tested to see if the artificial personas would vote the same as humans did in 2012, 2016, and 2020 U.S. presidential elections. Using the American National Election Studies (ANES) for their comparative human database, they found a high correspondence between how the AI and humans voted.\n\"I was absolutely surprised to see how accurately it matched up,\" said David Wingate, BYU computer science professor, and co-author on the study. \"It's especially interesting because the model wasn't trained to do political science -- it was just trained on a hundred billion words of text downloaded from the internet. But the consistent information we got back was so connected to how people really voted.\"\nIn another experiment, they conditioned artificial personas to offer responses from a list of options in an interview-style survey, again using the ANES as their human sample. They found high similarity between nuanced patterns in human and AI responses.\nThis innovation holds exciting prospects for researchers, marketers, and pollsters. Researchers envision a future where artificial intelligence is used to craft better survey questions, refining them to be more accessible and representative; and even simulate populations that are difficult to reach. It can be used to test surveys, slogans, and taglines as a precursor to focus groups.\n\"We're learning that AI can help us understand people better,\" said BYU political science professor Ethan Busby. \"It's not replacing humans, but it is helping us more effectively study people. It's about augmenting our ability rather than replacing it. It can help us be more efficient in our work with people by allowing us to pre-test our surveys and our messaging.\"\nAnd while the expansive possibilities of large language models are intriguing, the rise of artificial intelligence poses a host of questions -- how much does AI really know? Which populations will benefit from this technology and which will be negatively impacted? And how can we protect ourselves from scammers and fraudsters who will manipulate AI to create more sophisticated phishing scams?\nWhile much of that is still to be determined, the study lays out a set of criteria that future researchers can use to determine how accurate an AI model is for different subject areas.\n\"We're going to see positive benefits because it's going to unlock new capabilities,\" said Wingate, noting that AI can help people in many different jobs be more efficient. \"We're also going to see negative things happen because sometimes computer models are inaccurate and sometimes they're biased. It will continue to churn society.\"\nBusby says surveying artificial personas shouldn't replace the need to survey real people and that academics and other experts need to come together to define the ethical boundaries of artificial intelligence surveying in research related to social science."
  },
  "4": {
    "headline": "First silicon integrated ECRAM for a practical AI accelerator",
    "subtitle": null,
    "date_posted": "March 28, 2023",
    "source": "University of Illinois Grainger College of Engineering",
    "summary": "The transformative changes brought by deep learning and artificial intelligence are accompanied by immense costs. For example, OpenAI's ChatGPT algorithm costs at least $100,000 every day to operate. This could be reduced with accelerators, or computer hardware designed to efficiently perform the specific operations of deep learning. However, such a device is only viable if it can be integrated with mainstream silicon-based computing hardware on the material level.",
    "text": "This was preventing the implementation of one highly promising deep learning accelerator -- arrays of electrochemical random-access memory, or ECRAM -- until a research team at the University of Illinois Urbana-Champaign achieved the first material-level integration of ECRAMs onto silicon transistors. The researchers, led by graduate student Jinsong Cui and professor Qing Cao of the Department of Materials Science & Engineering, recently reported an ECRAM device designed and fabricated with materials that can be deposited directly onto silicon during fabrication in Nature Electronics, realizing the first practical ECRAM-based deep learning accelerator.\n\"Other ECRAM devices have been made with the many difficult-to-obtain properties needed for deep learning accelerators, but ours is the first to achieve all these properties and be integrated with silicon without compatibility issues,\" Cao said. \"This was the last major barrier to the technology's widespread use.\"\nECRAM is a memory cell, or a device that stores data and uses it for calculations in the same physical location. This nonstandard computing architecture eliminates the energy cost of shuttling data between the memory and the processor, allowing data-intensive operations to be performed very efficiently.\nECRAM encodes information by shuffling mobile ions between a gate and a channel. Electrical pulses applied to a gate terminal either inject ions into or draw ions from a channel, and the resulting change in the channel's electrical conductivity stores information. It is then read by measuring the electric current that flows across the channel. An electrolyte between the gate and the channel prevents unwanted ion flow, allowing ECRAM to retain data as a nonvolatile memory.\nThe research team selected materials compatible with silicon microfabrication techniques: tungsten oxide for the gate and channel, zirconium oxide for the electrolyte, and protons as the mobile ions. This allowed the devices to be integrated onto and controlled by standard microelectronics. Other ECRAM devices draw inspiration from neurological processes or even rechargeable battery technology and use organic substances or lithium ions, both of which are incompatible with silicon microfabrication.\nIn addition, the Cao group device has numerous other features that make it ideal for deep learning accelerators. \"While silicon integration is critical, an ideal memory cell must achieve a whole slew of properties,\" Cao said. \"The materials we selected give rise to many other desirable features.\"\nSince the same material was used for the gate and channel terminals, injecting ions into and drawing ions from the channel are symmetric operations, simplifying the control scheme and significantly enhancing reliability. The channel reliably held ions for hours at time, which is sufficient for training most deep neural networks. Since the ions were protons, the smallest ion, the devices switched quite rapidly. The researchers found that their devices lasted for over 100 million read-write cycles and were vastly more efficient than standard memory technology. Finally, since the materials are compatible with microfabrication techniques, the devices could be shrunk to the micro- and nanoscales, allowing for high density and computing power.\nThe researchers demonstrated their device by fabricating arrays of ECRAMs on silicon microchips to perform matrix-vector multiplication, a mathematical operation crucial to deep learning. Matrix entries, or neural network weights, were stored in the ECRAMs, and the array performed the multiplication on the vector inputs, represented as applied voltages, by using the stored weights to change the resulting currents. This operation as well as the weight update was performed with a high level of parallelism.\n\"Our ECRAM devices will be most useful for AI edge-computing applications sensitive to chip size and energy consumption,\" Cao said. \"That's where this type of device has the most significant benefits compared to what is possible with silicon-based accelerators.\"\nThe researchers are patenting the new device, and they are working with semiconductor industry partners to bring this new technology to market. According to Cao, a prime application of this technology is in autonomous vehicles, which must rapidly learn its surrounding environment and make decisions with limited computational resources. He is collaborating with Illinois electrical & computer engineering faculty to integrate their ECRAMs with foundry-fabricated silicon chips and Illinois computer science faculty to develop software and algorithms taking advantage of ECRAM's unique capabilities."
  },
  "5": {
    "headline": "AI 'brain' created from core materials for OLED TVs",
    "subtitle": null,
    "date_posted": "March 24, 2023",
    "source": "Pohang University of Science & Technology (POSTECH)",
    "summary": "A research team develops semiconductor devices for high-performance AI operations by applying IGZO materials widely used in OLED displays.",
    "text": "A research team at POSTECH, led by Professor Yoonyoung Chung (Department of Electrical Engineering, Department of Semiconductor Engineering), Professor Seyoung Kim (Department of Materials Science and Engineering, Department of Semiconductor Engineering), and Ph.D. candidate Seongmin Park (Department of Electrical Engineering), has developed a high-performance AI semiconductor device using indium gallium zinc oxide (IGZO), an oxide semiconductor widely used in OLED displays. The new device has proven to be excellent in terms of performance and power efficiency.\nEfficient AI operations, such as those of ChatGPT, require computations to occur within the memory responsible for storing information. Unfortunately, previous AI semiconductor technologies were limited in meeting all the requirements, such as linear and symmetric programming and uniformity, to improve AI accuracy.\nThe research team sought IGZO as a key material for AI computations that could be mass-produced and provide uniformity, durability, and computing accuracy. This compound comprises four atoms in a fixed ratio of indium, gallium, zinc, and oxygen and has excellent electron mobility and leakage current properties, which have made it a backplane of the OLED display.\nUsing this material, the researchers developed a novel synapse device composed of two transistors interconnected through a storage node. The precise control of this node's charging and discharging speed has enabled the AI semiconductor to meet the diverse performance metrics required for high-level performance. Furthermore, applying synaptic devices to a large-scale AI system requires the output current of synaptic devices to be minimized. The researchers confirmed the possibility of utilizing the ultra-thin film insulators inside the transistors to control the current, making them suitable for large-scale AI.\nThe researchers used the newly developed synaptic device to train and classify handwritten data, achieving a high accuracy of over 98%, which verifies its potential application in high-accuracy AI systems in the future.\nProfessor Chung explained, \"The significance of my research team's achievement is that we overcame the limitations of conventional AI semiconductor technologies that focused solely on material development. To do this, we utilized materials already in mass production. Furthermore, Linear and symmetrical programming characteristics were obtained through a new structure using two transistors as one synaptic device. Thus, our successful development and application of this new AI semiconductor technology show great potential to improve the efficiency and accuracy of AI.\"\nThis study was published last week on the inside back cover of Advanced Electronic Materials and was supported by the Next-Generation Intelligent Semiconductor Technology Development Program through the National Research Foundation, funded by the Ministry of Science and ICT of Korea."
  },
  "6": {
    "headline": "New in-home AI tool monitors the health of elderly residents",
    "subtitle": null,
    "date_posted": "March 23, 2023",
    "source": "University of Waterloo",
    "summary": "Engineers are harnessing artificial intelligence (AI) and wireless technology to unobtrusively monitor elderly people in their living spaces and provide early detection of emerging health problems.",
    "text": "The new system, built by researchers at the University of Waterloo, follows an individual's activities accurately and continuously as it gathers vital information without the need for a wearable device and alerts medical experts to the need to step in and provide help.\n\"After more than five years of working on this technology, we've demonstrated that very low-power, millimetre-wave radio systems enabled by machine learning and artificial intelligence can be reliably used in homes, hospitals and long-term care facilities,\" said Dr. George Shaker, an adjunct associate professor of electrical and computer engineering.\n\"An added bonus is that the system can alert healthcare workers to sudden falls, without the need for privacy-intrusive devices such as cameras.\"\nThe work by Shaker and his colleagues comes as overburdened public healthcare systems struggle to meet the urgent needs of rapidly growing elderly populations.\nWhile a senior's physical or mental condition can change rapidly, it's almost impossible to track their movements and discover problems 24/7 -- even if they live in long-term care. In addition, other existing systems for monitoring gait -- how a person walks -- are expensive, difficult to operate, impractical for clinics and unsuitable for homes.\nThe new system represents a major step forward and works this way: first, a wireless transmitter sends low-power waveforms across an interior space, such as a long-term care room, apartment or home.\nAs the waveforms bounce off different objects and the people being monitored, they're captured and processed by a receiver. That information goes into an AI engine which deciphers the processed waves for detection and monitoring applications.\nThe system, which employs extremely low-power radar technology, can be mounted simply on a ceiling or by a wall and doesn't suffer the drawbacks of wearable monitoring devices, which can be uncomfortable and require frequent battery charging.\n\"Using our wireless technology in homes and long-term care homes can effectively monitor various activities such as sleeping, watching TV, eating and the frequency of bathroom use,\" Shaker said.\n\"Currently, the system can alert care workers to a general decline in mobility, increased likelihood of falls, possibility of a urinary tract infection, and the onset of several other medical conditions.\"\nWaterloo researchers have partnered with a Canadian company, Gold Sentintel, to commercialize the technology, which has already been installed in several long-term care homes.\nA paper on the work, AI-Powered Non-Contact In-Home Gait Monitoring and Activity Recognition System Based on mm-Wave FMCW Radar and Cloud Computing, appears in the IEEE Internet of Things Journal.\nDoctoral student Hajar Abedi was the lead author, with contributions from Ahmad Ansariyan, Dr. Plinio Morita, Dr. Jen Boger and Dr. Alexander Wong."
  },
  "7": {
    "headline": "Optical switching at record speeds opens door for ultrafast, light-based electronics and computers",
    "subtitle": null,
    "date_posted": "March 22, 2023",
    "source": "University of Arizona",
    "summary": "Imagine a home computer operating 1 million times faster than the most expensive hardware on the market. Now, imagine that being the industry standard. Physicists hope to pave the way for that reality.",
    "text": "\"Semiconductor-based transistors are in all of the electronics that we use today,\" said Mohammed Hassan, assistant professor of physics and optical sciences. \"They're part of every industry -- from kids' toys to rockets -- and are the main building blocks of electronics.\"\nHassan lad an international team of researchers that published the research article \"Ultrafast optical switching and data encoding on synthesized light fields\" in Science Advances in February. UArizona physics postdoctoral research associate Dandan Hui and physics graduate student Husain Alqattan also contributed to the article, in addition to researchers from Ohio State University and the Ludwig Maximilian University of Munich.\nSemiconductors in electronics rely on electrical signals transmitted via microwaves to switch -- either allow or prevent -- the flow of electricity and data, represented as either \"on\" or \"off.\" Hassan said the future of electronics will be based instead on using laser light to control electrical signals, opening the door for the establishment of \"optical transistors\" and the development of ultrafast optical electronics.\nSince the invention of semiconductor transistors in the 1940s, technological advancement has centered on increasing the speed at which electric signals can be generated -- measured in hertz. According to Hassan, the fastest semiconductor transistors in the world can operate at a speed of more than 800 gigahertz. Data transfer at that frequency is measured at a scale of picoseconds, or one trillionth of a second.\nComputer processing power has increased steadily since the introduction of the semiconductor transistor, though Hassan said one of the primary concerns in developing faster technology is that the heat generated by continuing to add transistors to a microchip would eventually require more energy to cool than can pass through the chip.\nIn their article, Hassan and his collaborators discuss using all-optical switching of a light signal on and off to reach data transfer speeds exceeding a petahertz, measured at the attosecond time scale. An attosecond is one quintillionth of a second, meaning the transfer of data 1 million times faster than the fastest semiconductor transistors.\nWhile optical switches were already shown to achieve information processing speeds faster than that of semiconductor transistor-based technology, Hassan and his co-authors were able to register the on and off signals from a light source happening at the scale of billionths of a second. This was accomplished by taking advantage of a characteristic of fused silica, a glass often used in optics. Fused silica can instantaneously change its reflectivity, and by using ultrafast lasers, Hassan and his team were able to register changes in a light's signal at the attosecond time scale. The work also demonstrated the possibility of sending data in the form of \"one\" and \"zero\" representing on and off via light at previously impossible speeds.\n\"This new advancement would also allow the encoding of data on ultrafast laser pulses, which would increase the data transfer speed and could be used in long-distance communications from Earth into deep space,\" Hassan said. \"This promises to increase the limiting speed of data processing and information encoding and open a new realm of information technology.\"\nThe project was funded by a $1.4 million grant awarded to Hassan in 2018 by the Gordon and Betty Moore Foundation, an organization that aims \"to create positive outcomes for future generations\" by supporting research into scientific discovery, environmental conservation and patient care. The article was also based on work supported by the United States Air Force Office of Scientific Research's Young Investigator Research Program."
  },
  "8": {
    "headline": "Researchers create breakthrough spintronics manufacturing process that could revolutionize the electronics industry",
    "subtitle": "May lead to devices with 'unmatched' energy efficiency and memory storage density",
    "date_posted": "March 20, 2023",
    "source": "University of Minnesota",
    "summary": "Researchers have developed a breakthrough process for making spintronic devices that has the potential to create semiconductors chips with unmatched energy efficiency and storage for use in computers, smartphones, and many other electronics.",
    "text": "The researchers' paper is published in Advanced Functional Materials, a peer-reviewed, top-tier materials science journal.\n\"We believe we've found a material and a device that will allow the semiconducting industry to move forward with more opportunities in spintronics that weren't there before for memory and computing applications,\" said Jian-Ping Wang, senior author of the paper and professor and Robert F. Hartmann Chair in the University of Minnesota Department of Electrical and Computer Engineering. \"Spintronics is incredibly important for building microelectronics with new functionalities.\"\nWang said Minnesota has been leading this effort in a big way for more than 10 years with strong support by the Semiconductor Research Corporation (SRC), Defense Advanced Research Projects Agency (DARPA), and the National Science Foundation (NSF).\nWang's team has also worked with University of Minnesota Technology Commercialization and NIST to patent this technology, along with several other patents related to this research. This discovery also opens up a new vein of research for designing and manufacturing spintronic devices for the next decade.\n\"This means Honeywell, Skywater, Globalfoundries, Intel, and companies like them can integrate this material into their semiconductor manufacturing processes and products,\" Wang said. \"That's very exciting because engineers in the industry will be able to design even more powerful systems.\"\nThe semiconductor industry is constantly trying to develop smaller and smaller chips that can maximize energy efficiency, computing speed, and data storage capacity in electronic devices. Spintronic devices, which leverage the spin of electrons rather than the electrical charge to store data, provide a promising and more efficient alternative to traditional transistor-based chips. These materials also have the potential to be non-volatile, meaning they require less power and can store memory and perform computing even after you remove their power source.\nSpintronic materials have been successfully integrated into semiconductor chips for more than a decade now, but the industry standard spintronic material, cobalt iron boron, has reached a limit in its scalability. Currently, engineers are unable to make devices smaller than 20 nanometers without losing their ability to store data.\nThe University of Minnesota researchers have circumvented this problem by showing that iron palladium, an alternative material to cobalt iron boron that requires less energy and has the potential for more data storage, can be scaled down to sizes as small as five nanometers.\nAnd, for the first time, the researchers were able to grow iron palladium on a silicon wafer using an 8-inch wafer-capable multi-chamber ultrahigh vacuum sputtering system, a one-of-a-kind piece of equipment among academic institutions across the country and only available at the University of Minnesota.\n\"This work is showing for the first time in the world that you can grow this material, which can be scaled down to smaller than five nanometers, on top of a semiconductor industry-compatible substrate, so-called CMOS+X strategies,\" said Deyuan Lyu, first author on the paper and a Ph.D. student in the University of Minnesota Department of Electrical and Computer Engineering.\n\"Our team challenged ourselves to elevate a new material to manufacture spintronic devices needed for the next generation of data-hungry apps,\" said Daniel Gopman, a staff scientist at NIST and one of the key contributors to the research. \"It will be exciting to see how this advance drives further growth of spintronics devices within the semiconductor chip technology landscape.\"\nThis research was funded by a $4 million, four-year grant from DARPA and in part by NIST; SMART, one of seven centers of nCORE, an SRC program; and NSF.\nIn addition to Wang, Gopman, and Lyu, the research team comprised University of Minnesota researchers across the College of Science and Engineering, including Department of Electrical and Computer Engineering researchers Qi Jia, William Echtenkamp, and Brandon Zink; Department of Mechanical Engineering researcher Dingbin Huang and Associate Professor Xiaojia Wang; and Characterization Facility researchers Javier Garc\u00eda-Barriocanal, Geoffrey Rojas, and Guichuan Yu. National Institute of Standards and Technology researcher Jenae Shoup also contributed to the research."
  },
  "9": {
    "headline": "Superconducting amplifiers offer high performance with lower power consumption",
    "subtitle": null,
    "date_posted": "March 20, 2023",
    "source": "National Institutes of Natural Sciences",
    "summary": "Researchers have devised a new concept of superconducting microwave low-noise amplifiers for use in radio wave detectors for radio astronomy observations, and successfully demonstrated a high-performance cooled amplifier with power consumption three orders of magnitude lower than that of conventional cooled semiconductor amplifiers. This result is expected to contribute to the realization of large-scale multi-element radio cameras and error-tolerant quantum computers, both of which require a large number of low-noise microwave amplifiers.",
    "text": "The devise they used is called an SIS mixer. The SIS mixer is named after its structure, a very thin film of insulator material sandwiched between two layers of superconductors (S-I-S). In a radio telescope, cosmic radio waves collected by an antenna are fed into an SIS mixer, and the output signal is amplified by low-noise semiconductor amplifiers. An SIS mixer operates in a very low temperature environment, as low as 4 Kelvin (-269 degrees Celsius), and the amplifiers are also operated at that temperature.\nTo improve the performance of radio telescopes, researchers are developing a large-format radio camera equipped with 2D arrays of SIS mixers and amplifiers. However, the power consumption is a limiting factor. The typical power consumption of a semiconductor amplifier is about 10 mW, and by assembling 100 sets of detectors, the total power consumption reaches the maximum cooling capability of a 4 Kelvin refrigerator.\nThe research team led by Takafumi Kojima, an associate professor at the National Astronomical Observatory of Japan (NAOJ), has come up with a simple but innovative idea to realize a superconductor amplifier by connecting two SIS mixers. The team exploits the basic functions of the SIS mixer: frequency conversion and signal amplification. \"The most important point is that the power consumption of an SIS mixer is, in principle, as low as microwatts,\" says Kojima. \"This is three orders of magnitude less than that of a cooled semiconductor amplifier.\"\nAfter obtaining successful preliminary results in 2018, the team advanced both the theoretical studies of the system and the physical implementation of its various components. In the end, the research team optimized the system and realized an \"SIS amplifier\" with 5 -- 8 dB (three to six times) gain below the frequency of 5 GHz and a typical noise temperature of 10 K, which is comparable to the current cooled semiconductor amplifiers such as HEMT and HBT, but with much lower power consumption.\n\"By changing the configuration of the components, we can further improve the gain and low-noise performance of an SIS amplifier,\" explains Kojima. \"The idea of connecting two SIS mixers has broader applications for making various electronics that have functions other than amplification.\"\nInterestingly, this low-noise, low-power-consumption amplifier is also highly anticipated for large-scale error-tolerant quantum computers. Currently available quantum computers are small-scale with less than 100 qubits, but larger-scale, error-tolerant general-purpose quantum computers will require more than 1 million qubits. To handle a large number of qubits, a large number of amplifiers must also be installed, and dramatic reductions in amplifier power consumption are needed.\nNAOJ has experience in the development of superconducting receivers for a number of radio telescopes, including NAOJ's Nobeyama 45-meter Radio Telescope, which started operation in 1982. NAOJ is also currently working to upgrade the superconducting receivers to improve the performance of the Atacama Large Millimeter/submillimeter Array (ALMA), which is operated in the Republic of Chile in cooperation with East Asia, Europe, and North America. Of the 10 types of receivers (corresponding to 10 different frequency bands) currently installed on ALMA, three were developed by NAOJ, and the SIS chips at the heart of these receivers were also developed and produced in the cleanroom of the NAOJ Advanced Technology Center (ATC). The NAOJ ATC continues to promote research on the miniaturization and integration of superconducting circuits, not only for the realization of more powerful radio telescopes, but also for their potential as the basis of various technologies that will support society in the new era, such as quantum computing."
  },
  "10": {
    "headline": "Mind-control robots a reality?",
    "subtitle": null,
    "date_posted": "March 20, 2023",
    "source": "University of Technology Sydney",
    "summary": "Researchers have developed biosensor technology that will allow you to operate devices, such as robots and machines, solely through thought control.",
    "text": "The advanced brain-computer interface was developed by Distinguished Professor Chin-Teng Lin and Professor Francesca Iacopi, from the UTS Faculty of Engineering and IT, in collaboration with the Australian Army and Defence Innovation Hub.\nAs well as defence applications, the technology has significant potential in fields such as advanced manufacturing, aerospace and healthcare -- for example allowing people with a disability to control a wheelchair or operate prosthetics.\n\"The hands-free, voice-free technology works outside laboratory settings, anytime, anywhere. It makes interfaces such as consoles, keyboards, touchscreens and hand-gesture recognition redundant,\" said Professor Iacopi.\n\"By using cutting edge graphene material, combined with silicon, we were able to overcome issues of corrosion, durability and skin contact resistance, to develop the wearable dry sensors,\" she said.\nA new study outlining the technology has just been published in the peer-reviewed journal ACS Applied Nano Materials. It shows that the graphene sensors developed at UTS are very conductive, easy to use and robust.\nThe hexagon patterned sensors are positioned over the back of the scalp, to detect brainwaves from the visual cortex. The sensors are resilient to harsh conditions so they can be used in extreme operating environments.\nThe user wears a head-mounted augmented reality lens which displays white flickering squares. By concentrating on a particular square, the brainwaves of the operator are picked up by the biosensor, and a decoder translates the signal into commands.\nThe technology was recently demonstrated by the Australian Army, where soldiers operated a Ghost Robotics quadruped robot using the brain-machine interface. The device allowed hands-free command of the robotic dog with up to 94% accuracy.\n\"Our technology can issue at least nine commands in two seconds. This means we have nine different kinds of commands and the operator can select one from those nine within that time period,\" Professor Lin said.\n\"We have also explored how to minimise noise from the body and environment to get a clearer signal from an operator's brain,\" he said.\nThe researchers believe the technology will be of interest to the scientific community, industry and government, and hope to continue making advances in brain-computer interface systems."
  },
  "11": {
    "headline": "3D-printed revolving devices can sense how they are moving",
    "subtitle": null,
    "date_posted": "March 17, 2023",
    "source": "Massachusetts Institute of Technology",
    "summary": "Researchers created a system that enables makers to incorporate sensors directly into rotational mechanisms with only one pass in a 3D printer. This gives rotational mechanisms like gearboxes the ability to sense their angular position, rotation speed, and direction of rotation.",
    "text": "Even though advances in 3D printing enable rapid fabrication of rotational mechanisms, integrating sensors into the designs is still notoriously difficult. Due to the complexity of the rotating parts, sensors are typically embedded manually, after the device has already been produced.\nHowever, manually integrating sensors is no easy task. Embed them inside a device and wires might get tangled in the rotating parts or obstruct their rotations, but mounting external sensors would increase the size of a mechanism and potentially limit its motion.\nInstead, the new system the MIT researchers developed enables a maker to 3D print sensors directly into a mechanism's moving parts using conductive 3D printing filament. This gives devices the ability to sense their angular position, rotation speed, and direction of rotation.\nWith their system, called MechSense, a maker can manufacture rotational mechanisms with integrated sensors in just one pass using a multi-material 3D printer. These types of printers utilize multiple materials at the same time to fabricate a device.\nTo streamline the fabrication process, the researchers built a plugin for the computer-aided design software SolidWorks that automatically integrates sensors into a model of the mechanism, which could then be sent directly to the 3D printer for fabrication.\nMechSense could enable engineers to rapidly prototype devices with rotating parts, like turbines or motors, while incorporating sensing directly into the designs. It could be especially useful in creating tangible user interfaces for augmented reality environments, where sensing is critical for tracking a user's movements and interaction with objects.\n\"A lot of the research that we do in our lab involves taking fabrication methods that factories or specialized institutions create and then making then accessible for people. 3D printing is a tool that a lot of people can afford to have in their homes. So how can we provide the average maker with the tools necessary to develop these types of interactive mechanisms? At the end of the day, this research all revolves around that goal,\" says Marwa AlAlawi, a mechanical engineering graduate student and lead author of a paper on MechSense.\nAlAlawi's co-authors include Michael Wessely, a former postdoc in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) who is now an assistant professor at Aarhus University; and senior author Stefanie Mueller, an associate professor in the MIT departments of Electrical Engineering and Computer Science and Mechanical Engineering, and a member CSAIL; as well as others at MIT and collaborators from Accenture Labs. The research will be presented at the ACM CHI Conference on Human Factors in Computing Systems.\nBuilt-in sensing\nTo incorporate sensors into a rotational mechanism in a way that would not disrupt the device's movement, the researchers leveraged capacitive sensing.\nA capacitor consists of two plates of conductive material that have an insulating material sandwiched between them. If the overlapping area or distance between the conductive plates is changed, perhaps by rotating the mechanism, a capacitive sensor can detect resulting changes in the electric field between the plates. That information could then be used to calculate speed, for instance.\n\"In capacitive sensing, you don't necessarily need to have contact between the two opposing conductive plates to monitor changes in that specific sensor. We took advantage of that for our sensor design,\" AlAlawi says.\nRotational mechanisms typically consist of a rotational element located above, below, or next to a stationary element, like a gear spinning on a static shaft above a flat surface. The spinning gear is the rotational element and the flat surface beneath it is the stationary element.\nThe MechSense sensor includes three patches made from conductive material that are printed into the stationary plate, with each patch separated from its neighbors by nonconductive material. A fourth patch of conductive material, which has the same area as the other three patches, is printed into the rotating plate.\nAs the device spins, the patch on the rotating plate, called a floating capacitor, overlaps each of the patches on the stationary plate in turn. As the overlap between the rotating patch and each stationary patch changes (from completely covered, to half covered, to not covered at all), each patch individually detects the resulting change in capacitance.\nThe floating capacitor is not connected to any circuitry, so wires won't get tangled with rotating components.\nRather, the stationary patches are wired to electronics that use software the researchers developed to convert raw sensor data into estimations of angular position, direction of rotation, and rotation speed.\nEnabling rapid prototyping\nTo simplify the sensor integration process for a user, the researchers built a SolidWorks extension. A maker specifies the rotating and stationary parts of their mechanism, as well as the center of rotation, and then the system automatically adds sensor patches to the model.\n\"It doesn't change the design at all. It just replaces part of the device with a different material, in this case conductive material,\" AlAlawi says.\nThe researchers used their system to prototype several devices, including a smart desk lamp that changes the color and brightness of its light depending on how the user rotates the bottom or middle of the lamp. They also produced a planetary gearbox, like those that are used in robotic arms, and a wheel that measures distance as it rolls across a surface.\nAs they prototyped, the team also conducted technical experiments to fine-tune their sensor design. They found that, as they reduced the size of the patches, the amount of error in the sensor data increased.\n\"In an effort to generate electronic devices with very little e-waste, we want devices with smaller footprints that can still perform well. If we take our same approach and perhaps use a different material or manufacturing process, I think we can scale down while accumulating less error using the same geometry,\" she says.\nIn addition to testing different materials, AlAlawi and her collaborators plan to explore how they could increase the robustness of their sensor design to external noise, and also develop printable sensors for other types of moving mechanisms.\nThis research was funded, in part, by Accenture Labs."
  },
  "12": {
    "headline": "Qubits put new spin on magnetism: Boosting applications of quantum computers",
    "subtitle": "Experiments on a quantum annealing computer 'let the matter talk to you'",
    "date_posted": "March 17, 2023",
    "source": "DOE/Los Alamos National Laboratory",
    "summary": "Research using a quantum computer as the physical platform for quantum experiments has found a way to design and characterize tailor-made magnetic objects using quantum bits, or qubits. That opens up a new approach to develop new materials and robust quantum computing.",
    "text": "\"With the help of a quantum annealer, we demonstrated a new way to pattern magnetic states,\" said Alejandro Lopez-Bezanilla, a virtual experimentalist in the Theoretical Division at Los Alamos National Laboratory. Lopez-Bezanilla is the corresponding author of a paper about the research in Science Advances.\n\"We showed that a magnetic quasicrystal lattice can host states that go beyond the zero and one bit states of classical information technology,\" Lopez-Bezanilla said. \"By applying a magnetic field to a finite set of spins, we can morph the magnetic landscape of a quasicrystal object.\"\n\"A quasicrystal is a structure composed by the repetition of some basic shapes following rules different to those of regular crystals,\" he said.\nFor this work with Cristiano Nisoli, a theoretical physicist also at Los Alamos, a D-Wave quantum annealing computer served as the platform to conduct actual physical experiments on quasicrystals, rather than modeling them. This approach \"lets matter talk to you,\" Lopez-Bezanilla said, \"because instead of running computer codes, we go straight to the quantum platform and set all the physical interactions at will.\"\nThe ups and downs of qubits\nLopez-Bezanilla selected 201 qubits on the D-Wave computer and coupled them to each other to reproduce the shape of a Penrose quasicrystal.\nSince Roger Penrose in the 1970s conceived the aperiodic structures named after him, no one had put a spin on each of their nodes to observe their behavior under the action of a magnetic field.\n\"I connected the qubits so all together they reproduced the geometry of one of his quasicrystals, the so-called P3,\" Lopez-Bezanilla said. \"To my surprise, I observed that applying specific external magnetic fields on the structure made some qubits exhibit both up and down orientations with the same probability, which leads the P3 quasicrystal to adopt a rich variety of magnetic shapes.\"\nManipulating the interaction strength between qubits and the qubits with the external field causes the quasicrystals to settle into different magnetic arrangements, offering the prospect of encoding more than one bit of information in a single object.\nSome of these configurations exhibit no precise ordering of the qubits' orientation.\n\"This can play in our favor,\" Lopez-Bezanilla said, \"because they could potentially host a quantum quasiparticle of interest for information science.\" A spin quasiparticle is able to carry information immune to external noise.\nA quasiparticle is a convenient way to describe the collective behavior of a group of basic elements. Properties such as mass and charge can be ascribed to several spins moving as if they were one."
  },
  "13": {
    "headline": "Researcher solves nearly 60-year-old game theory dilemma",
    "subtitle": null,
    "date_posted": "March 14, 2023",
    "source": "University of California - Santa Cruz",
    "summary": "A researcher has solved a nearly 60-year-old game theory dilemma called the wall pursuit game, with implications for better reasoning about autonomous systems such as driver-less vehicles.",
    "text": "Dejan Milutinovic, professor of electrical and computer engineering at UC Santa Cruz, has long worked with colleagues on the complex subset of game theory called differential games, which have to do with game players in motion. One of these games is called the wall pursuit game, a relatively simple model for a situation in which a faster pursuer has the goal to catch a slower evader who is confined to moving along a wall.\nSince this game was first described nearly 60 years ago, there has been a dilemma within the game -- a set of positions where it was thought that no game optimal solution existed. But now, Milutinovic and his colleagues have proved in a new paper published in the journal IEEE Transactions on Automatic Control that this long-standing dilemma does not actually exist, and introduced a new method of analysis that proves there is always a deterministic solution to the wall pursuit game. This discovery opens the door to resolving other similar challenges that exist within the field of differential games, and enables better reasoning about autonomous systems such as driverless vehicles.\nGame theory is used to reason about behavior across a wide range of fields, such as economics, political science, computer science and engineering. Within game theory, the Nash equilibrium is one of the most commonly recognized concepts. The concept was introduced by mathematician John Nash and it defines game optimal strategies for all players in the game to finish the game with the least regret. Any player who chooses not to play their game optimal strategy will end up with more regret, therefore, rational players are all motivated to play their equilibrium strategy.\nThis concept applies to the wall pursuit game -- a classical Nash equilibrium strategy pair for the two players, the pursuer and evader, that describes their best strategy in almost all of their positions. However, there are a set of positions between the pursuer and evader for which the classical analysis fails to yield the game optimal strategies and concludes with the existence of the dilemma. This set of positions are known as a singular surface -- and for years, the research community has accepted the dilemma as fact.\nBut Milutinovic and his co-authors were unwilling to accept this.\n\"This bothered us because we thought, if the evader knows there is a singular surface, there is a threat that the evader can go to the singular surface and misuse it,\" Milutinovic said. \"The evader can force you to go to the singular surface where you don't know how to act optimally -- and then we just don't know what the implication of that would be in much more complicated games.\"\nSo Milutinovic and his coauthors came up with a new way to approach the problem, using a mathematical concept that was not in existence when the wall pursuit game was originally conceived. By using the viscosity solution of the Hamilton-Jacobi-Isaacs equation and introducing a rate of loss analysis for solving the singular surface they were able to find that a game optimal solution can be determined in all circumstances of the game and resolve the dilemma.\nThe viscosity solution of partial differential equations is a mathematical concept that was non-existent until the 1980s and offers a unique line of reasoning about the solution of the Hamilton-Jacobi-Isaacs equation. It is now well known that the concept is relevant for reasoning about optimal control and game theory problems.\nUsing viscosity solutions, which are functions, to solve game theory problems involves using calculus to find the derivatives of these functions. It is relatively easy to find game optimal solutions when the viscosity solution associated with a game has well-defined derivatives. This is not the case for the wall-pursuit game, and this lack of well-defined derivatives creates the dilemma.\nTypically when a dilemma exists, a practical approach is that players randomly choose one of possible actions and accept losses resulting from these decisions. But here lies the catch: if there is a loss, each rational player will want to minimize it.\nSo to find how players might minimize their losses, the authors analyzed the viscosity solution of the Hamilton-Jacobi-Isaacs equation around the singular surface where the derivatives are not well-defined. Then, they introduced a rate of loss analysis across these singular surface states of the equation. They found that when each actor minimizes its rate of losses, there are well-defined game strategies for their actions on the singular surface.\nThe authors found that not only does this rate of loss minimization define the game optimal actions for the singular surface, but it is also in agreement with the game optimal actions in every possible state where the classical analysis is also able to find these actions.\n\"When we take the rate of loss analysis and apply it elsewhere, the game optimal actions from the classical analysis are not impacted ,\" Milutinovic said. \"We take the classical theory and we augment it with the rate of loss analysis, so a solution exists everywhere. This is an important result showing that the augmentation is not just a fix to find a solution on the singular surface, but a fundamental contribution to game theory.\nMilutinovic and his coauthors are interested in exploring other game theory problems with singular surfaces where their new method could be applied. The paper is also an open call to the research community to similarly examine other dilemmas.\n\"Now the question is, what kind of other dilemmas can we solve?\" Milutinovic said."
  },
  "14": {
    "headline": "Cleaning up the atmosphere with quantum computing",
    "subtitle": "A quantum computing algorithm could identify better compounds for more efficient carbon capture.",
    "date_posted": "March 14, 2023",
    "source": "American Institute of Physics",
    "summary": "Practical carbon capture technologies are still in the early stages of development, with the most promising involving a class of compounds called amines that can chemically bind with carbon dioxide. Researchers now deploy an algorithm to study amine reactions through quantum computing. An existing quantum computer cab run the algorithm to find useful amine compounds for carbon capture more quickly, analyzing larger molecules and more complex reactions than a traditional computer can.",
    "text": "The amount of carbon dioxide in the atmosphere increases daily with no sign of stopping or slowing. Too much of civilization depends on the burning of fossil fuels, and even if we can develop a replacement energy source, much of the damage has already been done. Without removal, the carbon dioxide already in the atmosphere will continue to wreak havoc for centuries.\nAtmospheric carbon capture is a potential remedy to this problem. It would pull carbon dioxide out of the air and store it permanently to reverse the effects of climate change. Practical carbon capture technologies are still in the early stages of development, with the most promising involving a class of compounds called amines that can chemically bind with carbon dioxide. Efficiency is paramount in these designs, and identifying even slightly better compounds could lead to the capture of billions of tons of additional carbon dioxide.\nIn AVS Quantum Science, by AIP Publishing, researchers from the National Energy Technology Laboratory and the University of Kentucky deployed an algorithm to study amine reactions through quantum computing. The algorithm can be run on an existing quantum computer to find useful amine compounds for carbon capture more quickly.\n\"We are not satisfied with the current amine molecules that we use for this [carbon capture] process,\" said author Qing Shao. \"We can try to find a new molecule to do it, but if we want to test it using classical computing resources, it will be a very expensive calculation. Our hope is to have a fast algorithm that can screen thousands of new molecules and structures.\"\nAny computer algorithm that simulates a chemical reaction needs to account for the interactions between every pair of atoms involved. Even a simple three-atom molecule like carbon dioxide bonding with the simplest amine, ammonia, which has four atoms, results in hundreds of atomic interactions. This problem vexes traditional computers but is exactly the sort of question at which quantum computers excel.\nHowever, quantum computers are still a developing technology and are not powerful enough to handle these kinds of simulations directly. This is where the group's algorithm comes in: It allows existing quantum computers to analyze larger molecules and more complex reactions, which is vital for practical applications in fields like carbon capture.\n\"We are trying to use the current quantum computing technology to solve a practical environmental problem,\" said author Yuhua Duan."
  },
  "15": {
    "headline": "Researchers develop soft robot that shifts from land to sea with ease",
    "subtitle": "Highly dynamic bistable soft actuators allow for varied locomotion",
    "date_posted": "March 14, 2023",
    "source": "Carnegie Mellon University",
    "summary": "Most animals can quickly transition from walking to jumping to crawling to swimming if needed without reconfiguring or making major adjustments. Most robots cannot. But researchers have now created soft robots that can seamlessly shift from walking to swimming, for example, or crawling to rolling using a bistable actuator made of 3D-printed soft rubber containing shape-memory alloy springs that react to electrical currents by contracting, which causes the actuator to bend. The team used this bistable motion to change the actuator or robot's shape. Once the robot changes shape, it is stable until another electrical charge morphs it back to its previous configuration.",
    "text": "Most robots cannot. But researchers at Carnegie Mellon University have created soft robots that can seamlessly shift from walking to swimming, for example, or crawling to rolling.\n\"We were inspired by nature to develop a robot that can perform different tasks and adapt to its environment without adding actuators or complexity,\" said Dinesh K. Patel, a post-doctoral fellow in the Morphing Matter Lab in the School of Computer Science's Human-Computer Interaction Institute. \"Our bistable actuator is simple, stable and durable, and lays the foundation for future work on dynamic, reconfigurable soft robotics.\"\nThe bistable actuator is made of 3D-printed soft rubber containing shape-memory alloy springs that react to electrical currents by contracting, which causes the actuator to bend. The team used this bistable motion to change the actuator or robot's shape. Once the robot changes shape, it is stable until another electrical charge morphs it back to its previous configuration.\n\"Matching how animals transition from walking to swimming to crawling to jumping is a grand challenge for bio-inspired and soft robotics,\" said Carmel Majidi, a professor in the Mechanical Engineering Department in CMU's College of Engineering.\nFor example, one robot the team created has four curved actuators attached to the corners of a cellphone-sized body made of two bistable actuators. On land, the curved actuators act as legs, allowing the robot to walk. In the water, the bistable actuators change the robot's shape, putting the curved actuators in an ideal position to act as propellers so it can swim.\n\"You need to have legs to walk on land, and you need to have a propeller to swim in the water. Building a robot with separate systems designed for each environment adds complexity and weight,\" said Xiaonan Huang, an assistant professor of robotics at the University of Michigan and Majidi's former Ph.D. student. \"We use the same system for both environments to create an efficient robot.\"\nThe team created two other robots: one that can crawl and jump, and one inspired by caterpillars and pill bugs that can crawl and roll.\nThe actuators require only a hundred millisecond of electrical charge to change their shape, and they are durable. The team had a person ride a bicycle over one of the actuators a few times and changed their robots' shapes hundreds of times to demonstrate durability.\nIn the future, the robots could be used in rescue situations or to interact with sea animals or coral. Using heat-activated springs in the actuators could open up applications in environmental monitoring, haptics, and reconfigurable electronics and communication.\n\"There are many interesting and exciting scenarios where energy-efficient and versatile robots like this could be useful,\" said Lining Yao, the Cooper-Siegel Assistant Professor in HCII and head of the Morphing Matter Lab.\nThe team's research, \"Highly Dynamic Bistable Soft Actuator for Reconfigurable Multimodal Soft Robots,\" was featured on the cover of the January 2023 issue of Advanced Materials Technologies. The research team included co-first authors Patel and Huang; Yao; Majidi; Yichi Luo, a mechanical engineering master's student at CMU; and Mrunmayi Mungekar and M. Khalid Jawed, both from the Department of Mechanical and Aerospace Engineering at the University of California, Los Angeles."
  },
  "16": {
    "headline": "New kind of transistor could shrink communications devices on smartphones",
    "subtitle": null,
    "date_posted": "March 8, 2023",
    "source": "University of Michigan",
    "summary": "One month after announcing a ferroelectric semiconductor at the nanoscale thinness required for modern computing components, a team has now demonstrated a reconfigurable transistor using that material. Their work paves the way for single amplifiers that can do the work of multiple conventional amplifiers, among other possibilities.",
    "text": "The study is a featured article in Applied Physics Letters.\n\"By realizing this new type of transistor, it opens up the possibility for integrating multifunctional devices, such as reconfigurable transistors, filters and resonators, on the same platform -- all while operating at very high frequency and high power,\" said Zetian Mi, U-M professor of electrical and computer engineering who led the research, \"That's a game changer for many applications.\"\nAt its most basic level, a transistor is a kind of switch, letting an electric current through or preventing it from passing. The one demonstrated at Michigan is known as a ferroelectric high electron mobility transistor (FeHEMT) -- a twist on the HEMTs that can increase the signal, known as gain, as well as offering high switching speed and low noise. This makes them well suited as amplifiers for sending out signals to cell towers and Wi-Fi routers at high speeds.\nFerroelectric semiconductors stand out from others because they can sustain an electrical polarization, like the electric version of magnetism. But unlike a fridge magnet, they can switch which end is positive and which is negative. In the context of a transistor, this capability adds flexibility -- the transistor can change how it behaves.\n\"We can make our ferroelectric HEMT reconfigurable,\" said Ding Wang, a research scientist in electrical and computer engineering and first author of the study. \"That means it can function as several devices, such as one amplifier working as several amplifiers that we can dynamically control. This allows us to reduce the circuit area and lower the cost as well as the energy consumption.\"\nAreas of particular interest for this device are reconfigurable radio frequency and microwave communication as well as memory devices in next-generation electronics and computing systems.\n\"By adding ferroelectricity to an HEMT, we can make the switching sharper. This could enable much lower power consumption in addition to high gain, making for much more efficient devices,\" said Ping Wang, a research scientist in electrical and computer engineering and also the co-corresponding author of the research.\nThe ferroelectric semiconductor is made of aluminum nitride spiked with scandium, a metal sometimes used to fortify aluminum in performance bicycles and fighter jets. It is the first nitride-based ferroelectric semiconductor, enabling it to be integrated with the next-gen semiconductor gallium nitride. Offering speeds up to 100 times that of silicon, as well as high efficiency and low cost, gallium nitride semiconductors are contenders to displace silicon as the preferred material for electronic devices.\n\"This is a pivotal step toward integrating nitride ferroelectrics with mainstream electronics,\" Mi said.\nThe new transistor was grown using molecular beam epitaxy, the same approach used to make semiconductor crystals that drive the lasers in CD and DVD players.\nThe University of Michigan has applied for patent protection. Early work leading to this study was funded by the Office of Naval Research and the Blue Sky Initiative at the U-M College of Engineering.\nThe device was built in the Lurie Nanofabrication Facility and studied at the Michigan Center for Materials Characterization."
  },
  "17": {
    "headline": "Researchers take a step towards turning interactions that normally ruin quantum information into a way of protecting it",
    "subtitle": "A new method for predicting the behavior of quantum devices provides a crucial tool for real-world applications of quantum technology",
    "date_posted": "March 8, 2023",
    "source": "Aalto University",
    "summary": "A new method for predicting the behavior of quantum devices provides a crucial tool for real-world applications of quantum technology.",
    "text": "In a study published in Physical Review Letters, researchers at Aalto University in Finland and IAS Tsinghua University in China report a new way to predict how quantum systems, such as groups of particles, behave when they are connected to the external environment. Usually, connecting a system such as a quantum computer to its environment creates decoherence and leaks, which ruin any information about what's happening inside the system. Now, the researchers developed a technique which turns that problem into its a solution.\nThe research was carried out by Aalto doctoral researcher Guangze Chen under the supervision of Professor Jose Lado and in collaboration with Fei Song from IAS Tsinghua. Their approach combines techniques from two domains, quantum many-body physics and non-Hermitian quantum physics.\nProtection from decoherence and leaks\nOne of the most intriguing and powerful phenomena in quantum systems is many-body quantum correlations. Understanding these and predicting their behaviour is vital because they underpin the exotic properties of key components of quantum computers and quantum sensors. While a lot of progress has been made in predicting quantum correlations when matter is isolated from its environment, doing so when matter is coupled to its environment has so far eluded scientists.\nIn the new study, the team showed that connecting a quantum device to an external system can be a strength in the right circumstances. When a quantum device is host to so-called non-Hermitian topology, it leads to robustly protected quantum excitations whose resilience stems from the very fact that they are open to the environment. These kinds of open quantum systems can potentially lead to disruptive new strategies for quantum technologies that harness external coupling to protect information from decoherence and leaks.\nFrom idealised conditions to the real world\nThe study establishes a new theoretical method to calculate the correlations between quantum particles when they are coupled to their environment. 'The method we developed allows us to solve correlated quantum problems that present dissipation and quantum many-body interactions simultaneously. As a proof of concept, we demonstrated the methodology for systems with 24 interacting qubits featuring topological excitations,' says Chen.\nProfessor Lado explains that their approach will help move quantum research from idealised conditions to real-world applications. 'Predicting the behavior of correlated quantum matter is one of the critical problems for the theoretical design of quantum materials and devices. However, the difficulty of this problem becomes much greater when considering realistic situations in which quantum systems are coupled to an external environment. Our results represent a step forward in solving this problem, providing a methodology for understanding and predicting both quantum materials and devices in realistic conditions in quantum technologies,' he says."
  },
  "18": {
    "headline": "Artificial intelligence (AI) reconstructs motion sequences of humans and animals",
    "subtitle": null,
    "date_posted": "March 8, 2023",
    "source": "University of Konstanz",
    "summary": "Imagine for a moment, that we are on a safari watching a giraffe graze. After looking away for a second, we then see the animal lower its head and sit down. But, we wonder, what happened in the meantime? Computer scientists have found a way to encode an animal's pose and appearance in order to show the intermediate motions that are statistically likely to have taken place.",
    "text": "One key problem in computer vision is that images are incredibly complex. A giraffe can take on an extremely wide range of poses. On a safari, it is usually no problem to miss part of a motion sequence, but, for the study of collective behaviour, this information can be critical. This is where computer scientists with the new model \"neural puppeteer\" come in.\nPredictive silhouettes based on 3D points\n\"One idea in computer vision is to describe the very complex space of images by encoding only as few parameters as possible,\" explains Bastian Goldl\u00fccke, professor of computer vision at the University of Konstanz. One representation frequently used until now is the skeleton. In a new paper published in the Proceedings of the 16th Asian Conference on Computer Vision, Bastian Goldl\u00fccke and doctoral researchers Urs Waldmann and Simon Giebenhain present a neural network model that makes it possible to represent motion sequences and render full appearance of animals from any viewpoint based on just a few key points. The 3D view is more malleable and precise than the existing skeleton models.\n\"The idea was to be able to predict 3D key points and also to be able to track them independently of texture,\" says doctoral researcher Urs Waldmann. \"This is why we built an AI system that predicts silhouette images from any camera perspective based on 3D key points.\" By reversing the process, it is also possible to determine skeletal points from silhouette images. On the basis of the key points, the AI system is able to calculate the intermediate steps that are statistically likely. Using the individual silhouette can be important. This is because, if you only work with skeletal points, you would not otherwise know whether the animal you're looking at is a fairly massive one, or one that is close to starvation.\nIn the field of biology in particular, there are applications for this model: \"At the Cluster of Excellence 'Centre for the Advanced Study of Collective Behaviour', we see that many different species of animals are tracked and that poses also need to be predicted in this context,\" Waldmann says.\nLong-term goal: apply the system to as much data as possible on wild animals\nThe team started by predicting silhouette motions of humans, pigeons, giraffes and cows. Humans are often used as test cases in computer science, Waldmann notes. His colleagues from the Cluster of Excellence work with pigeons. However, their fine claws pose a real challenge. There was good model data for cows, while the giraffe's extremely long neck was a challenge that Waldmann was eager to take on. The team generated silhouettes based on a few key points -- from 19 to 33 in all.\nNow the computer scientists are ready for the real world application: In the University of Konstanz's Imaging Hanger, its largest laboratory for the study of collective behaviour, data will be collected on insects and birds in the future. In the Imaging Hangar, it is easier to control environmental aspects such as lighting or background than in the wild. However, the long-term goal is to train the model for as many species of wild animals as possible, in order to gain new insight into the behaviour of animals."
  },
  "19": {
    "headline": "Complex oxides could power the computers of the future",
    "subtitle": null,
    "date_posted": "March 7, 2023",
    "source": "University of Groningen",
    "summary": "Materials scientists describe in two papers how complex oxides can be used to create very energy-efficient magneto-electric spin-orbit (MESO) devices and memristive devices with reduced dimensions.",
    "text": "The development of classic silicon-based computers is approaching its limits. To achieve further miniaturization and to reduce energy consumption, different types of materials and architectures are required. Tamalika Banerjee, Professor of Spintronics of Functional Materials at the Zernike Institute for Advanced Materials, University of Groningen, is looking at a range of quantum materials to create these new devices. 'Our approach is to study these materials and their interfaces, but always with an eye on applications, such as memory or the combination of memory and logic.'\nMore efficient\nThe Banerjee group previously demonstrated how doped strontium titanate can be used to create memristors, which combine memory and logic. They have recently published two papers on devices 'beyond CMOS', the complementary metal oxide semiconductors which are the building blocks of present-day computer chips.\nOne candidate to replace CMOS is the magneto-electric spin-orbit (MESO) device, which could be 10 to 30 times more efficient. Several materials have been investigated for their suitability in creating such a device. Job van Rijn, a PhD student in the Banerjee group, is the first author of a paper in Physical Review B published in December 2022, describing how strontium manganate (SrMnO3 or SMO for short) might be a good candidate for MESO devices. 'It is a multiferroic material that couples spintronics and charge-based effects,' explains van Rijn. Spintronics is based on the spin (the magnetic moment) of electrons.\nBanerjee: 'The magnetic and charge orderings are coupled in this material, so we can switch magnetism with an electric field and polarization with a magnetic field.' And, importantly, these effects are present at temperatures close to room temperature. Van Rijn is investigating the strong coupling between the two effects. 'We know that ferromagnetism and ferroelectricity are tuneable by straining a thin SMO film. This straining was done by growing the films on different substrates.'\nStrain\nVan Rijn studies how strain induces ferroelectricity in the material and how it impacts the magnetic order. He analysed the domains in the strained films and noticed that magnetic interactions are greatly dependent on the crystal structure and, in particular, on oxygen vacancies, which modify the preferred direction of the magnetic order. 'Spin transport experiments lead us to the conclusion that the magnetic domains play an active role in the devices that are made of this material. Therefore, this study is the first step in establishing the potential use of strontium manganate for novel computing architectures.'\nOn 14 February, the Banerjee group published a second paper on devices 'beyond CMOS', in the journal Advanced Electronic Materials. PhD student Anouk Goossens is the first author of this paper on the miniaturization of memristors based on niobium-doped strontium titanate (SrTiO3 or STO). 'The number of devices per unit surface area is important,' says Goossens. 'But some memristor types are difficult to downscale.'\nGoossens previously showed that it was possible to create 'logic-in-memory' devices using STO. Her latest paper shows that it is possible to downscale these devices. A common problem with memristors is that their performance is negatively impacted by miniaturization. Surprisingly, making smaller memristors from STO increases the difference between the high and the low resistance ratio. 'We studied the material using scanning transmission electron microscopy and noticed the presence of a large number of oxygen vacancies at the interface between the substrate and the device's electrode', says Goossens. 'After we applied an electric voltage, we noticed oxygen vacancy movement, which is a key factor in controlling the resistance states.'\nNew design\nThe conclusion is that the enhanced performance results from edge effects, which can be bad for normal memory. But in STO, the increased electric field at the edges actually supports the function of the memristor. 'In our case, the edge is the device,' concludes Goossens. 'In addition, the exact properties depend on the amount of niobium doping, so the material is tuneable for different purposes.'\nIn conclusion, both papers published by the group show the way towards novel computing architectures. Indeed, the STO memristors have inspired colleagues of Goossens and Banerjee at the University of Groningen Bernoulli Institute for Mathematics, Computer Science and Artificial Intelligence and CogniGron (Groningen Cognitive Systems and Materials Center), who have already come up with a new design for memory architecture.\n'This is exactly what we are working for,' says Banerjee. 'We want to understand the physics of materials and the way in which our devices work and then develop applications.' Goosens: 'We envision several applications and the one we are looking at is a random number generator that works without an algorithm and is therefore impossible to predict.'"
  },
  "20": {
    "headline": "Phone-based measurements provide fast, accurate information about the health of forests",
    "subtitle": null,
    "date_posted": "March 6, 2023",
    "source": "University of Cambridge",
    "summary": "Researchers have developed an algorithm that uses computer vision techniques to accurately measure trees almost five times faster than traditional, manual methods.",
    "text": "The researchers, from the University of Cambridge, developed the algorithm, which gives an accurate measurement of tree diameter, an important measurement used by scientists to monitor forest health and levels of carbon sequestration.\nThe algorithm uses low-cost, low-resolution LiDAR sensors that are incorporated into many mobile phones, and provides results that are just as accurate, but much faster, than manual measurement techniques. The results are reported in the journal Remote Sensing.\nThe primary manual measurement used in forest ecology is tree diameter at chest height. These measurements are used to make determinations about the health of trees and the wider forest ecosystem, as well as how much carbon is being sequestered.\nWhile this method is reliable, since the measurements are taken from the ground, tree by tree, the method is time-consuming. In addition, human error can lead to variations in measurements.\n\"When you're trying to figure out how much carbon a forest is sequestering, these ground-based measurements are hugely valuable, but also time-consuming,\" said first author Amelia Holcomb from Cambridge's Department of Computer Science and Technology. \"We wanted to know whether we could automate this process.\"\nSome aspects of forest measurement can be carried out using expensive special-purpose LiDAR sensors, but Holcomb and her colleagues wanted to determine whether these measurements could be taken using cheaper, lower-resolution sensors, of the type that are used in some mobile phones for augmented reality applications.\nOther researchers have carried out some forest measurement studies using this type of sensor, however this has been focused on highly-managed forests where trees are straight, evenly spaced and undergrowth is regularly cleared. Holcomb and her colleagues wanted to test whether these sensors could return accurate results for non-managed forests quickly, automatically, and in a single image.\n\"We wanted to develop an algorithm that could be used in more natural forests, and that could deal with things like low-hanging branches, or trees with natural irregularities,\" said Holcomb.\nThe researchers designed an algorithm that uses a smartphone LiDAR sensor to estimate trunk diameter automatically from a single image in realistic field conditions. The algorithm was incorporated into a custom-built app for an Android smartphone, and is able to return results in near real-time.\nTo develop the algorithm, the researchers first collected their own dataset by measuring trees manually and taking pictures. Using image processing and computer vision techniques, they were able to train the algorithm to differentiate trunks from large branches, determine which direction trees were leaning in, and other information that could help it refine the information about forests.\nThe researchers tested the app in three different forests -- one each in the UK, US and Canada -- in spring, summer and autumn. The app was able to detect 100% of tree trunks, and had a mean error rate of 8%, which is comparable to the error rate when measuring by hand. However, the app sped up the process significantly, and was about four and a half times faster than measuring trees manually.\n\"I was surprised the app works as well as it does,\" said Holcomb. \"Sometimes I like to challenge it with a particularly crowded bit of forest, or a particularly oddly-shaped tree, and I think there's no way it will get it right, but it does.\"\nSince their measurement tool requires no specialised training and uses sensors that are already incorporated into an increasing number of phones, the researchers say that it could be an accurate, low-cost tool for forest measurement, even in complex forest conditions.\nThe researchers plan to make their app publicly available for Android phones later this spring.\nThe research was supported in part by the David Cheriton Graduate Scholarship, the Canadian National Research Council, and the Harding Distinguished Postgraduate Scholarship."
  }
}